{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i61xFkNOLNal"
   },
   "source": [
    "# Bayes Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u0iBaufLQ4p"
   },
   "source": [
    "In this notebook we will demonstrate how to calculate probabilities and distributions directly from our data. We will do this *by hand* rather than using the scikit-learn functions, just because then we know what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkFbDwNZLJpN"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78iLG3nrLc4Q"
   },
   "source": [
    "These are standard imports for working with the data and loading our Google Drive (which contains our CSV file) -- if your data is stored on your local PC, you'll have to edit this accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwsalJ10rPoJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We use two different plotting libraries, depending on which kind of plot we want\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set an option for Pandas to display smaller floating-point numbers\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Need to get Google Drive access\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OYt2Tx4Lqjl"
   },
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAciTHvoL3S0"
   },
   "source": [
    "## Peeking at the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fBwM2i6Ls2o"
   },
   "source": [
    "Remember, you should always quickly double-check that your data was loaded successfully by taking a peek at what your dataframe contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dufcX3HJrjXt"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas dataframe\n",
    "data_dir = os.path.join('/content/gdrive/My Drive/classes/be432-2021/notebooks/wisconsin_breast_cancer_data.csv')\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKNP8qSorlrj"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdjM8itIL0ZM"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU6-pmfGL2QX"
   },
   "source": [
    "We create a new column that contains numeric values for each of the unique classes in our dataset.\n",
    "\n",
    "\"Malignant\" is coded as \"M\" in the original data, and 1 in the new label column; \"Benign\" is \"B\" in the original, and 0 in the encoded data -- double-check to be sure! \n",
    "\n",
    "Note that the numeric values of the labels are assigned alphabetically -- there's no way for the program to know that \"Malignant\" is normally called \"positive\" (i.e. 1) and \"Benign\" is \"negative\" (0). It just so happens that B comes before M in the alphabet and it starts from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Kk15FfrmFb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "diagnosis_cat = df['diagnosis']\n",
    "\n",
    "# Fit the encoder to the categories, and immediately \n",
    "diagnosis_lab = label_encoder.fit_transform(diagnosis_cat)\n",
    "\n",
    "# Add the diagnosis label back to the dataframe\n",
    "df['diagnosis_label'] = diagnosis_lab\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db4CWg_RMo7C"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2lPYa0ZMq7k"
   },
   "source": [
    "Before we do anything else, we create our training and testing sets. We set a test size of 30%, and split on the diagnosis label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O17Ffk1qrsjy"
   },
   "outputs": [],
   "source": [
    "# Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create the splitting object\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply the split to the data frame using the \"diagnosis\" column as our label\n",
    "for train_index, test_index in split.split(df, df[\"diagnosis_label\"]):\n",
    "    train_set = df.loc[train_index]\n",
    "    test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pyv0GlquLBiz"
   },
   "outputs": [],
   "source": [
    "test_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s93NtFZHMyMH"
   },
   "source": [
    "For simplicitly, we separate out the feature values from the diagnosis label. We don't actually care about the patient IDs at this point (although if you're doing a more involved ML project, you probably want to trace back from your misclassifications to your sample IDs).\n",
    "\n",
    "We also separate the values into separate class-specific dataframes as well; this helps to plot things out later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiZnAkDcrt4z"
   },
   "outputs": [],
   "source": [
    "training_values = train_set.drop(['id','diagnosis', 'diagnosis_label'], axis=1)\n",
    "training_labels = train_set[['diagnosis_label']].copy()\n",
    "\n",
    "testing_values = test_set.drop(['id','diagnosis', 'diagnosis_label'], axis=1)\n",
    "testing_labels = test_set[['diagnosis_label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g36AIjeztcPB"
   },
   "outputs": [],
   "source": [
    "# Separate out our testing data into classes for easier plotting\n",
    "malignant = training_values.loc[training_labels['diagnosis_label'] == 1,:]\n",
    "benign = training_values.loc[training_labels['diagnosis_label'] == 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RC2dOKCpt2Lv"
   },
   "outputs": [],
   "source": [
    "benign.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CgzNAJPNRnD"
   },
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws3_PX2UNYCo"
   },
   "source": [
    "In this section we create the various distributions that we'll be using to calculate probabilities.\n",
    "\n",
    "First, select a feature to look at. We'll only look at a single feature in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZF7u_FfNjji"
   },
   "outputs": [],
   "source": [
    "feat_name = 'texture_mean'\n",
    "\n",
    "# Create a nice-looking name to use in the plot\n",
    "feat_display = feat_name.replace('_', ' ').title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJst5bX-Oksx"
   },
   "source": [
    "## Create a Density Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r86AyOZNnib"
   },
   "source": [
    "Only four lines of code below are used to actually calculate the distribution: `ax.hist(...)` does most of the work, while the `np.append(...)` function is necessary to make the number of counts and bins match up.\n",
    "\n",
    "Each `ax.hist(...)` line returns the following:\n",
    "\n",
    "- `counts` is the number of samples in each bin of the histogram.\n",
    "- `bins` contains the edges of each bin; the first value is the left-hand side of the left-most bin, and the last one is the right-hand side of the right-most bin. \n",
    "\n",
    "The rest of the code is justcreating and styling the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlBLqHfwuHmf"
   },
   "outputs": [],
   "source": [
    "# Create the holder for the plots\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "m_counts, m_bins, _ = ax.hist(malignant[feat_name], density=True, alpha=.8, label=\"Malignant\", bins=10, edgecolor='k')\n",
    "b_counts, b_bins, _ = ax.hist(benign[feat_name], density=True, alpha=.8, label=\"Benign\",  bins=10, edgecolor='k')\n",
    "\n",
    "# We need to pad counts with a zero because bins is larger by 1\n",
    "m_counts = np.append(m_counts, [0])\n",
    "b_counts = np.append(b_counts, [0])\n",
    "\n",
    "#ax.set(xlim=(5,40))\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x$: '+feat_display,\n",
    "       ylabel=r'$p(x|\\omega_{j})$',\n",
    "       title='Probability Density Function (PDF) for '+feat_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9otw-akEOWTd"
   },
   "source": [
    "Note that on the `y`-axis, we are plotting $p(x|\\omega_{i})$ rather than the actual number of samples that fall into that bin; this is because we passed `density=True` to the `ax.hist(...)` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvQGqdeaOn8S"
   },
   "source": [
    "## Examine Histogram Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik4v40DUOqCg"
   },
   "source": [
    "Let's take a look at what the histogram actually calculates. This is the *percentage* of samples (because we asked for a density) in each class that fall within each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tOt7zw9z0aU"
   },
   "outputs": [],
   "source": [
    "# Examine the outputs\n",
    "print(f'Number of samples in each bin:')\n",
    "print(f'\\t\\tBin\\tCount')\n",
    "print(f'')\n",
    "for m_count, m_bin in zip(m_counts, m_bins):\n",
    "  print(f'Malignant\\t{m_bin:2.2f}\\t{m_count:1.3f}')\n",
    "\n",
    "print(f'='*29)\n",
    "\n",
    "print(f'\\t\\tBin\\tCount')\n",
    "print(f'')\n",
    "for b_count, b_bin in zip(b_counts, b_bins):\n",
    "  print(f'Benign\\t\\t{b_bin:2.2f}\\t{b_count:1.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KtoQzpZO3Cw"
   },
   "source": [
    "# Calculating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI3AohFIQfGG"
   },
   "source": [
    "## Probability Density Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctwjSy_0O5Lz"
   },
   "source": [
    "Let's try to calcualte $p(x|\\omega_{i})$ for both $\\omega_{0}$ (Benign) and  $\\omega_{1}$ (Malignant) for a *specific* value of $x$.\n",
    "\n",
    "For example, what if we measure a feature value of 20? What's the probability of observing a 20 from the benign vs. the malignant classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpakdVim0Och"
   },
   "outputs": [],
   "source": [
    "feature_value = 20\n",
    "#feature_value = 0.07\n",
    "\n",
    "# First we find the bin associated with this value\n",
    "# This will be the first bin that is greater than our target value\n",
    "m_locs = m_bins > feature_value\n",
    "m_idx = np.min(np.where(m_locs))\n",
    "\n",
    "print(f'Feature value {feature_value} appears in the {m_idx}th bin of the Malignant histogram.')\n",
    "\n",
    "b_locs = b_bins > feature_value\n",
    "b_idx = np.min(np.where(b_locs))\n",
    "\n",
    "print(f'Feature value {feature_value} appears in the {b_idx}th bin of the Benign histogram.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6oxf7N_Pg_P"
   },
   "source": [
    "Now, we can just look at the corresponding bin in each histogram and pull the percentage of samples in each class that have that feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY4PHxTGPqKe"
   },
   "outputs": [],
   "source": [
    "p_x_given_m = m_counts[m_idx]\n",
    "p_x_given_b = b_counts[b_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wtxK-po23xh"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Math, HTML\n",
    "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
    "               \"latest.js?config=default'></script>\"))\n",
    "Math(r'$p(x|\\omega_{1}) = '+f'{p_x_given_m:2.3f}'+r'$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXU_p9k84PGo"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Math, HTML\n",
    "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
    "               \"latest.js?config=default'></script>\"))\n",
    "Math(r'$p(x|\\omega_{2}) = '+f'{p_x_given_b:2.3f}'+r'$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWmrBtrReUKs"
   },
   "source": [
    "### Refactor to a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QluCUl4P_xk"
   },
   "source": [
    "Let's create a function that will return the histogram counts (density) for any value. \n",
    "\n",
    "This is *very common* in programming -- whenever you anticipate you'll have to do something more than once, it's a good idea to write a function to do it for you. It takes more time up front, but saves a ton of time later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEJSO-N-7qNk"
   },
   "outputs": [],
   "source": [
    "def p_x_given_omega(bins, counts, value):\n",
    "  \"\"\"Given a set of bins and corresponding histogram counts, finds the counts associated with a specific value.\"\"\"\n",
    "  \n",
    "  assert len(bins) == len(counts), f'Bins has length of {len(bins)}, counts has length of {len(counts)}; these should match.'\n",
    "  locs = bins > value\n",
    "\n",
    "  # If none of the locations are larger than the value, then return zero\n",
    "  if np.any(locs):\n",
    "    idx = np.min(np.where(locs))\n",
    "    return counts[idx]\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAMWMJiAeYAy"
   },
   "source": [
    "## Prior Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVeDgwBDeZfL"
   },
   "source": [
    "The prior in this case is \"what is the likelihood that we observe a benign or malignant case, regardless of the feature data?\"\n",
    "\n",
    "This is often difficult to calculate. For our case, we'll just set them up as non-informative priors:\n",
    "\n",
    "$$p(\\omega_{0}) = p(\\omega_{1}) = 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ6xspzk4P3g"
   },
   "outputs": [],
   "source": [
    "# Prior probabilities\n",
    "p_m = 0.5\n",
    "p_b = 1 - p_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2a5xlWqetlC"
   },
   "source": [
    "## \"Evidence\" or \"Normalizing Factor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTfrnYRPewcp"
   },
   "source": [
    "The *a posteriori* probability must be normalized. To do this, we sum up all the possible ways we could observe a particular feature value:\n",
    "\n",
    "1. When the class is $\\omega_{0}$; or \n",
    "2. When the class is $\\omega_{1}$\n",
    "\n",
    "To do this, we first need to set up the x-axis that will be our feature \"domain\", or the set of values that we want to calculate over. Mathematically this should be $-\\infty$ to $+\\infty$, but for our case we can just go from the minimum observed feature value to the maximum feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHh7GKd_4vaK"
   },
   "outputs": [],
   "source": [
    "# Set up the bounds of our domain by calculating the min and max feature values\n",
    "x_min = np.min([malignant[feat_name].min(), benign[feat_name].min()])\n",
    "x_max = np.max([malignant[feat_name].max(), benign[feat_name].max()])\n",
    "\n",
    "print(f'Minimum value for {feat_name}: {x_min:2.2f}')\n",
    "print(f'Maximum value for {feat_name}: {x_max:2.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFD4kuPE5q3v"
   },
   "outputs": [],
   "source": [
    "# Now create a vector that goes from min to max, representing different feature values we could measure.\n",
    "x_domain = np.arange(x_min, x_max, 0.001)\n",
    "\n",
    "# Our step size is 0.01, which should be good enough for our purposes.\n",
    "print(f'X domain for {feat_name}:')\n",
    "print(x_domain)\n",
    "print('')\n",
    "print(f'Number of samples in our x axis: {len(x_domain)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anHA9e7p6iup"
   },
   "source": [
    "Now for each of these $x$ values, calculate our denominator value:\n",
    "\n",
    "$$ \\sum_{i=0}^{c} P(x|\\omega_{i})P(\\omega_{i}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnYCb-OI5swQ"
   },
   "outputs": [],
   "source": [
    "denominator = []\n",
    "for x in x_domain:\n",
    "  denominator.append(p_x_given_omega(m_bins, m_counts, x)*p_m + p_x_given_omega(b_bins, b_counts, x)*p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKOZPfGBflzh"
   },
   "source": [
    "## *A Posteriori* Probability of Observing $\\omega_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vngONYeGfqgu"
   },
   "source": [
    "Finally we can calculate $P(\\omega_{i} | x)$ for each value of $x$ in our domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tn17AB98DT9"
   },
   "outputs": [],
   "source": [
    "p_m_given_x = []\n",
    "p_b_given_x = []\n",
    "for i,x in enumerate(x_domain):\n",
    "  p_m_given_x.append( (p_x_given_omega(m_bins, m_counts, x) * p_m) / denominator[i] )\n",
    "  p_b_given_x.append( (p_x_given_omega(b_bins, b_counts, x) * p_b) / denominator[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO8SsnZx-5aR"
   },
   "outputs": [],
   "source": [
    "# Create the holder for the plots\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "\n",
    "ax.plot(x_domain, p_m_given_x, alpha=0.6, linewidth=5, label=r'$p(\\omega_{1} | x)$ (Malignant)')\n",
    "ax.plot(x_domain, p_b_given_x, alpha=0.6, linewidth=5, label=r'$p(\\omega_{0} | x)$ (Benign)')\n",
    "\n",
    "#ax.set(xlim=(5,40))\n",
    "ax.set(xlim=(x_domain.min(), x_domain.max()))\n",
    "\n",
    "# Annotate Plot\n",
    "ax.set(xlabel=r'$x$ domain for feature: '+feat_display,\n",
    "       ylabel=r'$p(\\omega_{j} | x)$',\n",
    "       title='A Posteriori Probability for '+feat_display)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wYJn2CkicrM"
   },
   "source": [
    "Note that this output is very \"blocky\" -- that's because for all of the samples within a bin, we have the same output value.\n",
    "\n",
    "What were to happen if we use a **parametric** probability density function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52V76SqlhVCX"
   },
   "source": [
    "# The Cheaty Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMK2BtQIhW2T"
   },
   "source": [
    "Of course, you could just... cheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcVqVdIchYrA"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(np.array(training_values[feat_name]).reshape(-1, 1), training_labels).predict_proba(x_domain.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyabZxsVs4u1"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnMrwDLChsff"
   },
   "outputs": [],
   "source": [
    "# Create the holder for the plots\n",
    "f, axes = plt.subplots(1,2,figsize=(20,6))\n",
    "\n",
    "axes[0].plot(x_domain, p_m_given_x, alpha=0.6, linewidth=5, label=r'$p(\\omega_{1} | x)$ (Malignant)')\n",
    "axes[0].plot(x_domain, p_b_given_x, alpha=0.6, linewidth=5, label=r'$p(\\omega_{0} | x)$ (Benign)')\n",
    "\n",
    "axes[1].plot(x_domain, y_pred[:,1], alpha=0.6, linewidth=5, label=r'$p(\\omega_{1} | x)$ (Malignant)')\n",
    "axes[1].plot(x_domain, y_pred[:,0], alpha=0.6, linewidth=5, label=r'$p(\\omega_{0} | x)$ (Benign)')\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "  ax.set(xlim=(5,40))\n",
    "\n",
    "  # Annotate Plot\n",
    "  ax.set(xlabel=r'$x$ domain for feature: '+feat_display,\n",
    "        ylabel=r'$p(\\omega_{j} | x)$',\n",
    "        title='A Posteriori Probability for '+feat_display)\n",
    "\n",
    "  ax.legend(frameon=True)\n",
    "  ax.grid(linestyle=':')\n",
    "  plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCBOaGKTjE_U"
   },
   "source": [
    "What are the similarities between these curves? What are the differences?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "i61xFkNOLNal",
    "SkFbDwNZLJpN",
    "4OYt2Tx4Lqjl",
    "MAciTHvoL3S0",
    "YdjM8itIL0ZM",
    "db4CWg_RMo7C",
    "8CgzNAJPNRnD",
    "DJst5bX-Oksx",
    "WvQGqdeaOn8S",
    "4KtoQzpZO3Cw",
    "VI3AohFIQfGG",
    "eWmrBtrReUKs",
    "yAMWMJiAeYAy",
    "x2a5xlWqetlC",
    "zKOZPfGBflzh",
    "52V76SqlhVCX"
   ],
   "name": "02-bayes-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
